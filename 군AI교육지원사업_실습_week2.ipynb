{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKLsF1d0ssK9"
   },
   "source": [
    "# 군AI교육지원사업\n",
    "+ **자연어 처리**\n",
    "+ **실습자료**\n",
    "> [Google Colab](https://colab.research.google.com/) - 파일 - 노트 열기 - Github - `JoSangYeon/CAU_mAI_NLP`입력 - `군AI교육지원사업_실습_week2.ipynb` 클릭\n",
    ">\n",
    "> ![image](https://user-images.githubusercontent.com/28241676/232968481-36737e8e-15ca-4a98-a230-070a3c901312.png)\n",
    "\n",
    "---\n",
    "\n",
    "## WEEK 2\n",
    "### 1. Assigment (week1)\n",
    "### 2. Word Embedding\n",
    ">2-1. Sparse vs Dense\n",
    ">\n",
    ">2-2. Word2Vec\n",
    ">\n",
    "> ** Assignment(week2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlQytSVFC8_F"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HO4ZaTfFsdk8"
   },
   "source": [
    "## 1. Assignment(week1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bq3Kv3ibtGhk"
   },
   "source": [
    "**[문제] 'low', 'lower', 'newest', 'widest'에 대하여 BPE를 사용해서 단어집합을 구성하였다. 이때 새로 등장한 단어 'lowest'에 대하여 토큰화를 수행해보자.**\n",
    "\n",
    "**[Hint]**\n",
    "\n",
    "1.   lowest를 글자 단위로 분해 → l, o, w, e, s, t\n",
    "2.   앞서 업데이트된 단어집합을 사용해서 pair를 통합\n",
    "(이때, 통합을 위한 우선순위는 bpe_codes를 사용(value값이 작을수록 대응하는 key의 우선순위가 높음))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aI2YP_W7tG23"
   },
   "outputs": [],
   "source": [
    "bpe_codes = {('e', 's'): 0,\n",
    "             ('es', 't'): 1,\n",
    "             ('l', 'o'): 2,\n",
    "             ('lo', 'w'): 3,\n",
    "             ('n', 'e'): 4,\n",
    "             ('ne', 'w'): 5,\n",
    "             ('new', 'est'): 6,\n",
    "             ('w', 'i'): 7,\n",
    "             ('wi', 'd'): 8,\n",
    "             ('wid', 'est'): 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oH9cS6ztOV4"
   },
   "outputs": [],
   "source": [
    "def get_pairs(word):                 # input: \"lowest\"\n",
    "\n",
    "    pairs = set()\n",
    "    prev_char = word[0]              # l\n",
    "\n",
    "    for char in word[1:]:            #[owest]\n",
    "        pairs.add((prev_char, char)) #('l', 'o')  ##('o', 'w')\n",
    "        prev_char = char             # o          ## w\n",
    "    return pairs                     #final: {('e', 's'), ('l', 'o'), ('o', 'w'), ('s', 't'), ('w', 'e')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5wQkhtUtTp7"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "def encode(orig):\n",
    "\n",
    "    word = tuple(orig)    # lowest → (l,o,w,e,s,t)\n",
    "    display(Markdown(\"__word split into characters:__ <tt>{}</tt>\".format(word)))\n",
    "\n",
    "    pairs = get_pairs(word)    # make unigram pairs of the word {('e', 's'), ('l', 'o'), ('o', 'w'), ('s', 't'), ('w', 'e')}\n",
    "\n",
    "    if not pairs:    # 예를 들어, 'l'이 input으로 들어온 경우 pairs가 빈 집합을 반환 (빈 시퀀스 = False)\n",
    "        return orig\n",
    "        \n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        display(Markdown(\"__Iteration {}:__\".format(iteration)))\n",
    "\n",
    "        print(\"bigrams in the word: {}\".format(pairs))\n",
    "        bigram = min(pairs, key = lambda pair: bpe_codes.get(pair, float('inf')))    # bpe_codes.get(pair, float('inf)): bpe_codes에서, pair에 대한 value 값. 딕셔너리에 존재하지 않는 pair면, value값으로 float('inf')를 반환.\n",
    "        print(\"candidate for merging: {}\".format(bigram))                                # bigram=('e', 's')                                 ## bigram=('es', 't')                ### bigram=('l','o')               #### bigram=('lo','w')\n",
    "\n",
    "        if bigram not in bpe_codes:\n",
    "            display(Markdown(\"__Candidate not in BPE merges, algorithm stops.__\"))\n",
    "            break\n",
    "\n",
    "        first, second = bigram                                                           # first='e', second='s'                             ## first='es', second='t'     \n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):                                                             # len(word)=6                                       ## len(word)=5                                                                         \n",
    "            try: \n",
    "                j = word.index(first, i)    # i번째 위치부터 처음 first가 위치한 자리    # j=3                                               ## j=3                         \n",
    "                new_word.extend(word[i:j])                                               # new_word=['l', 'o', 'w']                          ## new_word=['l', 'o', 'w']                                              \n",
    "                i = j             \n",
    "            except:\n",
    "                new_word.extend(word[i:])                                                #    new_word=['l', 'o', 'w', 'es', 't']            ##           \n",
    "                break\n",
    "            try:\n",
    "                if word[i] == first and word[i+1] == second and i < len(word)-1:\n",
    "                    new_word.append(first+second)                                        # new_word=['l', 'o', 'w', 'es']                    ## new_words=['l', 'o', 'w', 'est']\n",
    "                    i += 2                                                               # i=5                                               ## i=5                             \n",
    "            except:    # lowest의 경우는 문제x, loweste의 경우는 문제o\n",
    "                new_word.append(word[i])\n",
    "                break\n",
    "\n",
    "        new_word = tuple(new_word)                                                       # new_word=('l', 'o', 'w', 'es', 't')               ## new_word=('l', 'o', 'w', 'est') \n",
    "        word = new_word                                                                  # word=('l', 'o', 'w', 'es', 't')                   ## word=('l', 'o', 'w', 'est')     \n",
    "\n",
    "        print(\"word after merging: {}\".format(word))\n",
    "        if len(word) == 1:    # merging을 반복해서, 초기 input과 같은 형태가 된 경우(더 이상 merge 할게 없는 경우)\n",
    "            break\n",
    "        else:\n",
    "            pairs = get_pairs(word)                                                      # {('es', 't'),('l', 'o'),('o', 'w'),('w', 'es')}   ## {('l','o'), ('o','w'), ('w','est')}\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "id": "C8k86lQWtVbX",
    "outputId": "978d9727-4e5f-4331-8e03-a24ff97284ea"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "__word split into characters:__ <tt>('l', 'o', 'w', 'e', 's', 't')</tt>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 1:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('l', 'o'), ('e', 's'), ('o', 'w'), ('w', 'e'), ('s', 't')}\n",
      "candidate for merging: ('e', 's')\n",
      "word after merging: ('l', 'o', 'w', 'es', 't')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 2:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('l', 'o'), ('o', 'w'), ('es', 't'), ('w', 'es')}\n",
      "candidate for merging: ('es', 't')\n",
      "word after merging: ('l', 'o', 'w', 'est')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 3:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('l', 'o'), ('o', 'w'), ('w', 'est')}\n",
      "candidate for merging: ('l', 'o')\n",
      "word after merging: ('lo', 'w', 'est')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 4:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('lo', 'w'), ('w', 'est')}\n",
      "candidate for merging: ('lo', 'w')\n",
      "word after merging: ('low', 'est')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 5:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('low', 'est')}\n",
      "candidate for merging: ('low', 'est')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Candidate not in BPE merges, algorithm stops.__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('low', 'est')"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"lowest\")   # iteration별 출력 예시 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWxdfk6QtekK"
   },
   "source": [
    "# 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZxrG4bM43NQ"
   },
   "source": [
    "## 2-1. Sparse vs Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c29AGT0B7H40"
   },
   "source": [
    "**[Limitation of Sparse Representaion]**\n",
    "\n",
    "* 단어의 개수가 늘어날수록, 벡터의 차원이 증가\n",
    "\n",
    "* 단어의 유사도 표현 불가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tD_N2ll5Nf1"
   },
   "source": [
    "**[Example of Sparse Represenation: One-Hot Encoding]**\n",
    "\n",
    "* 표현 대상 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식\n",
    "\n",
    "* 표현된 벡터**(One-Hot Vector)**의 차원 = 단어 집합(vocabulary)의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmVhnm23xTAU"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bzw7N_MA-4pm",
    "outputId": "ec6af74f-dfee-4ac4-8cc5-2ad3742bbedc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-Hot Vector 생성\n",
    "\n",
    "def one_hot(vocab):\n",
    "    encoding_matrix = torch.zeros(len(vocab), len(vocab))    # vocabulary의 차원만큼 원소 0을 갖는 텐서 만들기\n",
    "    for index, word in enumerate(vocab):\n",
    "        encoding_matrix[index,index] = 1\n",
    "    return encoding_matrix\n",
    "\n",
    "vocabulary = ['cat','dog','computer','netbook']\n",
    "one_hot_encoding = one_hot(vocabulary)\n",
    "one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ysrJpaXA5pV0",
    "outputId": "09ec3dd8-42a9-4e02-cb31-4e05c0acdfa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog:  tensor([1., 0., 0., 0.])\n",
      "cat:  tensor([0., 1., 0., 0.])\n",
      "computer:  tensor([0., 0., 1., 0.])\n",
      "netbook:  tensor([0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "dog = one_hot_encoding[:, 0]\n",
    "cat = one_hot_encoding[:, 1]\n",
    "computer = one_hot_encoding[:, 2]\n",
    "netbook = one_hot_encoding[:, 3]\n",
    "\n",
    "print('dog: ', dog) #label, encoding pair\n",
    "print('cat: ', cat)\n",
    "print('computer: ', computer)\n",
    "print('netbook: ', netbook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZjM0ZL9q6V-9",
    "outputId": "88ae13d4-9506-4d8f-d027-c831155231b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(torch.cosine_similarity(cat, dog, dim=0))\n",
    "print(torch.cosine_similarity(cat, computer, dim=0))\n",
    "print(torch.cosine_similarity(computer, netbook, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eST2va2C9b3w"
   },
   "source": [
    "<img src = \"https://d33wubrfki0l68.cloudfront.net/43d0db23e9f1bfed7a3f34acbf1abacc7f831309/a72c9/images/nlp-embedding-methods-2.jpg\" height = 300 width = 540>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJfowR_h6gLO"
   },
   "source": [
    "## 2-2. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5Zn2MX7ZJeW"
   },
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4wljPod0Qv7"
   },
   "outputs": [],
   "source": [
    "## using pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y41MU5yLAUpy"
   },
   "source": [
    "<img src = \"https://upload.wikimedia.org/wikipedia/commons/a/a2/Cbow.png\" height = 420 width = 360>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpVj_yP4457H"
   },
   "outputs": [],
   "source": [
    "## CBOW 모델 정의\n",
    "\n",
    "embedding_dim = 128\n",
    "embedding_max_norm = 1   # Embedding Layer의 weight가 너무 커지지 않도록 제한\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, embedding_max_norm)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input):         # input.shape: [4]\n",
    "        x = self.embeddings(input)    # [4,128] (=[4,48]x[48,128])\n",
    "        x = sum(x)                    # [128]                          # sum or mean(v=(sum of context_vectors)/number of context_size))\n",
    "        x = x.view(1,-1)              # [1, 128]\n",
    "        x = self.linear(x)            # [1, 48] (=[1,128]x[128,48])\n",
    "        x = F.softmax(x, dim=-1)      # [1, 48]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQziOVqUVDmQ"
   },
   "outputs": [],
   "source": [
    "example_sentence = \"In the case of CBOW, one word is eliminated, and the word is predicted from surrounding words. Therefore, it takes multiple input vectors as inputs to the model and creates one output vector. In contrast, Skip-Gram learns by removing all words except one word and predicting the surrounding words in the context through one word. So, it takes a vector as input and produces multiple output vectors. CBOW and Skip-Gram are different.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgKXVIR3N9Jl"
   },
   "outputs": [],
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "# Tokenization (just split)\n",
    "tokenized_sentence = example_sentence.split()\n",
    "\n",
    "# Make Vocabulary\n",
    "vocab = set(tokenized_sentence)    # 중복제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aU-GRUN420BA",
    "outputId": "1331222f-ab3d-4da8-a8e7-b4fcacdc7330"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CBOW',\n",
       " 'CBOW,',\n",
       " 'In',\n",
       " 'Skip-Gram',\n",
       " 'So,',\n",
       " 'Therefore,',\n",
       " 'a',\n",
       " 'all',\n",
       " 'and',\n",
       " 'are',\n",
       " 'as',\n",
       " 'by',\n",
       " 'case',\n",
       " 'context',\n",
       " 'contrast,',\n",
       " 'creates',\n",
       " 'different.',\n",
       " 'eliminated,',\n",
       " 'except',\n",
       " 'from',\n",
       " 'in',\n",
       " 'input',\n",
       " 'inputs',\n",
       " 'is',\n",
       " 'it',\n",
       " 'learns',\n",
       " 'model',\n",
       " 'multiple',\n",
       " 'of',\n",
       " 'one',\n",
       " 'output',\n",
       " 'predicted',\n",
       " 'predicting',\n",
       " 'produces',\n",
       " 'removing',\n",
       " 'surrounding',\n",
       " 'takes',\n",
       " 'the',\n",
       " 'through',\n",
       " 'to',\n",
       " 'vector',\n",
       " 'vector.',\n",
       " 'vectors',\n",
       " 'vectors.',\n",
       " 'word',\n",
       " 'word.',\n",
       " 'words',\n",
       " 'words.'}"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dFFBYPPfJ2X8",
    "outputId": "42ecfc8b-ff6d-4049-f9fd-5fac4dfade92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vector.': 0, 'input': 1, 'context': 2, 'predicted': 3, 'it': 4, 'inputs': 5, 'Skip-Gram': 6, 'word': 7, 'Therefore,': 8, 'through': 9}\n",
      "{0: 'vector.', 1: 'input', 2: 'context', 3: 'predicted', 4: 'it', 5: 'inputs', 6: 'Skip-Gram', 7: 'word', 8: 'Therefore,', 9: 'through'}\n"
     ]
    }
   ],
   "source": [
    "## Make Dictionary _ word2index{word: index}, index2word{index: word}\n",
    "\n",
    "word2index = {word:index for index, word in enumerate(vocab)}\n",
    "index2word = {index:word for index, word in enumerate(vocab)}\n",
    "\n",
    "import itertools\n",
    "print(dict(itertools.islice(word2index.items(),10)))\n",
    "print(dict(itertools.islice(index2word.items(),10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNhLEmSwLC6d"
   },
   "outputs": [],
   "source": [
    "## Make Dataset\n",
    "\n",
    "def make_data(tokenized_sentence):\n",
    "    data = []\n",
    "    for i in range(2, len(tokenized_sentence) - 2):    #context_size=4\n",
    "        input = [tokenized_sentence[i - 2], tokenized_sentence[i - 1], \n",
    "                tokenized_sentence[i + 1], tokenized_sentence[i + 2]]   # context 범위 설정(window size = 2)\n",
    "        target = tokenized_sentence[i]    # target 지정\n",
    "        data.append((input, target))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "prWqWnLIPIYY",
    "outputId": "9ed51575-f4a0-4781-98f7-b4d1e5746f4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  ['In', 'the', 'of', 'CBOW,']\n",
      "target:  case\n"
     ]
    }
   ],
   "source": [
    "data = make_data(example_sentence.split())\n",
    "print(\"input: \", data[0][0])\n",
    "print(\"target: \", data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3XOrENdJ8na"
   },
   "outputs": [],
   "source": [
    "## Convert Context to Index Vector\n",
    "def make_index_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxVtJbjr0tc5",
    "outputId": "6161d3fa-0b64-4ae6-86b4-75aa7146c02a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13, 26, 21, 37])\n",
      "tensor([14])\n"
     ]
    }
   ],
   "source": [
    "print(make_index_vector(data[0][0], word2index ))\n",
    "print(make_index_vector([data[0][1]], word2index ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jsQYwqaM8Xc-",
    "outputId": "886d5e5e-142b-45fa-9b2c-fd6778a5b9c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0 , loss =  tensor(263.2672, grad_fn=<AddBackward0>)\n",
      "epoch =  50 , loss =  tensor(257.4954, grad_fn=<AddBackward0>)\n",
      "epoch =  100 , loss =  tensor(248.9848, grad_fn=<AddBackward0>)\n",
      "epoch =  150 , loss =  tensor(244.4839, grad_fn=<AddBackward0>)\n",
      "epoch =  200 , loss =  tensor(239.1063, grad_fn=<AddBackward0>)\n",
      "epoch =  250 , loss =  tensor(236.4796, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Train\n",
    "\n",
    "model = CBOW(len(vocab), embedding_dim)    # 모델 선언\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS=300\n",
    "\n",
    "#input_, target_ = make_data(example_sentence.split())  \n",
    "#input = torch.LongTensor(input_)\n",
    "#target = torch.LongTensor(target_)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for input, target in data:\n",
    "        input_ = make_index_vector(input, word2index)\n",
    "        output = model(input_)\n",
    "        total_loss += loss_function(output, torch.tensor([word2index[target]]))\n",
    "    if epoch % 50 == 0:\n",
    "        print('epoch = ',epoch, ', loss = ',total_loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5ZVAatrKaem",
    "outputId": "d6b15f64-6a2a-4f1c-bd83-771b039c5333"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction :  Skip-Gram\n"
     ]
    }
   ],
   "source": [
    "## Test\n",
    "\n",
    "test_data = ['CBOW','and','are','different.']\n",
    "test_vector = make_index_vector(test_data, word2index)\n",
    "result = model(test_vector)\n",
    "print('Prediction : ', index2word[torch.argmax(result[0]).item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nmynNTJ0-2oK",
    "outputId": "a4054972-6a08-45c9-d16c-e5d5efb01270"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('it', tensor(13.6805)),\n",
       " ('word', tensor(14.7983)),\n",
       " ('model', tensor(14.8115)),\n",
       " ('words', tensor(14.8679)),\n",
       " ('vectors', tensor(14.8957)),\n",
       " ('Skip-Gram', tensor(15.0138))]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Embedding 결과\n",
    "\n",
    "target_word='CBOW'\n",
    "\n",
    "embeddings = model.embeddings.weight.data\n",
    "word_embedding = embeddings[word2index[target_word]]    #128D\n",
    "\n",
    "distances = []\n",
    "for word, index in word2index.items():\n",
    "    if word == target_word:    # target_word(대상 word) 제외\n",
    "        continue\n",
    "    distances.append((word, torch.dist(word_embedding, embeddings[index]).item()))\n",
    "n=5\n",
    "results = sorted(distances, key=lambda x: x[1])[1:n+2]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eSUImt29KG4"
   },
   "source": [
    "## **Assignment(week2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezIo1700BBJw"
   },
   "source": [
    "**[과제]**\n",
    "\n",
    "### Skip-Gram 구현하기\n",
    "\n",
    "**[HINT]**\n",
    "- 자유롭게 코드 구현 가능(하이퍼파라미터, 모델 세부 구조 등 전부 자유)\n",
    "- 아래의 가이드라인 코드를 참고해서, 빈칸('''CODE''' 부분)만 채워도 상관없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9nduh6y45t-"
   },
   "source": [
    "### Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03u3GjFu4-9Q"
   },
   "source": [
    "<img src = \"https://upload.wikimedia.org/wikipedia/commons/9/95/Skip-gram.png\" height = 420 width = 360>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9mEW9IUBYay"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "자유롭게 코드 작성\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNVevRcsBYX-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvzwVPuQBXch"
   },
   "source": [
    "**[가이드라인]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIXFiQp1-pge"
   },
   "outputs": [],
   "source": [
    "## using pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3RMeSS14-Zn"
   },
   "outputs": [],
   "source": [
    "## Skip-Gram 모델 정의\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "embedding_dim = 128\n",
    "embedding_max_norm = 1   # Embedding Layer의 weight가 너무 커지지 않도록 제한\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size: int, context_size=4):\n",
    "        super(SkipGram, self).__init__()\n",
    "        '''\n",
    "        Enter Your CODE\n",
    "        '''\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Enter Your CODE\n",
    "        '''\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q5AqXQPuDCBZ",
    "outputId": "11c0b63a-dace-427b-a8b6-5382096acc3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In the case of CBOW, one word is eliminated, and the word is predicted from surrounding words',\n",
       " 'Therefore, it takes multiple input vectors as inputs to the model and creates one output vector',\n",
       " 'In contrast, Skip-Gram learns by removing all words except one word and predicting the surrounding words in the context through one word',\n",
       " ' So, it takes a vector as input and produces multiple output vectors',\n",
       " 'CBOW and Skip-Gram are different.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = \"In the case of CBOW, one word is eliminated, and the word is predicted from surrounding words. Therefore, it takes multiple input vectors as inputs to the model and creates one output vector. In contrast, Skip-Gram learns by removing all words except one word and predicting the surrounding words in the context through one word.  So, it takes a vector as input and produces multiple output vectors. CBOW and Skip-Gram are different.\" \n",
    "example_data = example_sentence.split('. ')\n",
    "example_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8GO_i3S180bR"
   },
   "outputs": [],
   "source": [
    "# Tokenization (just split)\n",
    "tokenized_sentence = []\n",
    "for sentence in example_data:\n",
    "    tokenized_sentence.append(sentence.split())\n",
    "\n",
    "# Make Vocabulary\n",
    "vocab = set([token for sentence in tokenized_sentence for token in sentence])    # 중복제거\n",
    "#-----------------------------------------------------------------------\n",
    "## Make Dictionary _ word2index{word: index}, index2word{index: word}\n",
    "\n",
    "word2index = {word:index for index, word in enumerate(vocab)}\n",
    "index2word = {index:word for index, word in enumerate(vocab)}\n",
    "#-----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6j2sID7-Fa8"
   },
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset) :\n",
    "    def __init__(self, train_tokenized, vocab, window_size = 2) :\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        \n",
    "        for tokens in train_tokenized :\n",
    "            token_ids = [vocab[token] for token in tokens]\n",
    "            for i, id in enumerate(token_ids) :\n",
    "                if i-window_size >= 0 and i+window_size < len(token_ids) :\n",
    "                    self.y += (token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1]) #y:context\n",
    "                    self.x += [id] * 2 * window_size #x:center(* context개수)\n",
    "        self.x = torch.LongTensor(self.x)\n",
    "        self.y = torch.LongTensor(self.y)\n",
    "    \n",
    "    def __len__(self) :\n",
    "        return self.x.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx) :\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0SSbIYxDmgU"
   },
   "outputs": [],
   "source": [
    "skipgram_set = SkipGramDataset(tokenized_sentence, word2index)\n",
    "\n",
    "batch_size = 4\n",
    "loader = DataLoader(skipgram_set, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAj9OvrL-tLU"
   },
   "outputs": [],
   "source": [
    "# ('case', ['In', 'the', 'of', 'CBOW,']\n",
    "## ('case', 'In'),\n",
    "## ('case', 'the'),\n",
    "## ('case', 'of'),\n",
    "## ('case', 'CBOW,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-KpEIOUl-tIv"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = SkipGram(vocab_size = len(word2index), dim = 256).to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WuZRMCl1-FYX",
    "outputId": "0410e225-3bc6-47ef-8e91-569197e5abff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train loss per epoch: 4.233072757720947\n",
      "Epoch: 2\n",
      "Train loss per epoch: 4.1801581382751465\n",
      "Epoch: 3\n",
      "Train loss per epoch: 4.128326416015625\n",
      "Epoch: 4\n",
      "Train loss per epoch: 4.077564239501953\n",
      "Epoch: 5\n",
      "Train loss per epoch: 4.027857780456543\n",
      "Epoch: 6\n",
      "Train loss per epoch: 3.9791953563690186\n",
      "Epoch: 7\n",
      "Train loss per epoch: 3.931565523147583\n",
      "Epoch: 8\n",
      "Train loss per epoch: 3.8849592208862305\n",
      "Epoch: 9\n",
      "Train loss per epoch: 3.839369058609009\n",
      "Epoch: 10\n",
      "Train loss per epoch: 3.7947890758514404\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-4\n",
    "num_epochs = 10\n",
    "\n",
    "model.train()\n",
    "model = model\n",
    "\n",
    "for e in range(1, num_epochs + 1) :\n",
    "    print(f\"Epoch: {e}\")\n",
    "    '''\n",
    "    Enter Your CODE\n",
    "    '''\n",
    "    print(f\"Train loss per epoch: {loss.item():.6f}\")\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAlsQWiW--v_"
   },
   "source": [
    "-- END --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvGE-28yDH6A"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "[reference]\n",
    "\n",
    "https://srijithr.gitlab.io/post/word2vec/\n",
    "\n",
    "https://github.com/OlgaChernytska/word2vec-pytorch\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
