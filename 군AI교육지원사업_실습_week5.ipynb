{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyODUFbCBtM6jfg3BR/gvY4s"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"fbc07594bc874f2cb160bc1e01079c48":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a64715b8f48a4c8da41a01865130fdad","IPY_MODEL_db931f21783d47eab023fb005eb6d707","IPY_MODEL_3ac5b265ab5a4304abfcdf22701ea44a"],"layout":"IPY_MODEL_77e4bdc74ca94d7da3f66470d14ed7e7"}},"a64715b8f48a4c8da41a01865130fdad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0860302d87b484da331082547d90bdf","placeholder":"​","style":"IPY_MODEL_dd32680f5beb4ad49af35f75a33faa3f","value":"100%"}},"db931f21783d47eab023fb005eb6d707":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a7e73438c924e73884d7352611782ed","max":1377,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b8a48b237f754c71a9bb5bdef0528167","value":1377}},"3ac5b265ab5a4304abfcdf22701ea44a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c01c5ef492dc483db1145de6d21f81a3","placeholder":"​","style":"IPY_MODEL_81b8bb88a827494cbfa078c363959287","value":" 1377/1377 [02:54&lt;00:00,  7.50it/s]"}},"77e4bdc74ca94d7da3f66470d14ed7e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0860302d87b484da331082547d90bdf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd32680f5beb4ad49af35f75a33faa3f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a7e73438c924e73884d7352611782ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8a48b237f754c71a9bb5bdef0528167":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c01c5ef492dc483db1145de6d21f81a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81b8bb88a827494cbfa078c363959287":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d9d896ac5574105af7b64d6805150b2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_191a4b58634b4145a832347ea71121f0","IPY_MODEL_f99ec136d3c843cab7a9cfb151696e03","IPY_MODEL_a90b621f3a8846e5a2c5aa30b56cbe70"],"layout":"IPY_MODEL_de6118d5ece14045abbe0a1607531451"}},"191a4b58634b4145a832347ea71121f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8a27c82a0514500afe7280affb60243","placeholder":"​","style":"IPY_MODEL_31ccaf8b70ec4e7681393b63da163550","value":"Downloading builder script: "}},"f99ec136d3c843cab7a9cfb151696e03":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e64fca755860403daee1d9cdbea127f3","max":1844,"min":0,"orientation":"horizontal","style":"IPY_MODEL_56979b6301a54831ac09975641747f17","value":1844}},"a90b621f3a8846e5a2c5aa30b56cbe70":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40e8246ae67949709ef035a9885bf233","placeholder":"​","style":"IPY_MODEL_6f060530ed0045afacc853c0a1f3dfe3","value":" 5.76k/? [00:00&lt;00:00, 137kB/s]"}},"de6118d5ece14045abbe0a1607531451":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8a27c82a0514500afe7280affb60243":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31ccaf8b70ec4e7681393b63da163550":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e64fca755860403daee1d9cdbea127f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56979b6301a54831ac09975641747f17":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"40e8246ae67949709ef035a9885bf233":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f060530ed0045afacc853c0a1f3dfe3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# 1. Assignment Solution (week4)"],"metadata":{"id":"MHK3SH53ksl2"}},{"cell_type":"markdown","source":["**[before]** charcater-level RNN\n","\n","appl → pple"],"metadata":{"id":"SZqcKsKVkvio"}},{"cell_type":"markdown","source":["**[Assigment]** word-level RNN\n","\n","Repeat is the best medicine for → is the best medicine for memory\n","\n","- Hint: one-hot vector 대신 embedding 사용"],"metadata":{"id":"yw2GadSVkxAB"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import ipdb"],"metadata":{"id":"lItH0ULwkzYv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentence = \"Repeat is the best medicine for memory\".split()\n","vocab = list(set(sentence))\n","print(vocab)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KyLlMKE0k0ub","executionInfo":{"status":"ok","timestamp":1700414072191,"user_tz":-540,"elapsed":398,"user":{"displayName":"Jang Jang","userId":"00176447522705973821"}},"outputId":"76ddbb0d-51e9-407d-b2ff-b7b834dff4e6"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["['medicine', 'Repeat', 'best', 'for', 'is', 'memory', 'the']\n"]}]},{"cell_type":"markdown","source":["**Word2idx**"],"metadata":{"id":"0QT38DpQk8Iq"}},{"cell_type":"code","source":["word2index = {tkn: i for i, tkn in enumerate(vocab, 1)}     # 단어에 고유한 정수 부여\n","word2index['<unk>']=0\n","print(word2index)\n","print(word2index['memory'])"],"metadata":{"id":"vNwkG6Hmk9zI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Idx2word**"],"metadata":{"id":"Du_7UHZBlI1A"}},{"cell_type":"code","source":["index2word = {v: k for k, v in word2index.items()}\n","print(index2word)\n","print(index2word[2])"],"metadata":{"id":"7XbSpdKslK75"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Build Data**"],"metadata":{"id":"0f_wDjiPlOAI"}},{"cell_type":"code","source":["def build_data(sentence, word2index):\n","    encoded = [word2index[token] for token in sentence]    # word → index\n","    input_seq, label_seq = encoded[:-1], encoded[1:]    # input sequence, label sequence\n","    input_seq = torch.LongTensor(input_seq).unsqueeze(0)    # 차원 하나 추가 (for batch)\n","    label_seq = torch.LongTensor(label_seq).unsqueeze(0)    # 라벨의 차원 하나 추가(for batch)\n","    return input_seq, label_seq\n","X, Y = build_data(sentence, word2index)\n","print('input sequence:', X)    # (Repeat is the best medicine for)\n","print('label sequence:', Y)    # (is the best medicine for memory)"],"metadata":{"id":"eaW4gKV0lPLk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Model**"],"metadata":{"id":"gQmFgEHdlTf1"}},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_first=True):\n","        super(Net, self).__init__()\n","        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)     # for word embedding\n","        self.rnn_layer = nn.RNN(embedding_dim, hidden_dim, batch_first=batch_first)\n","        self.linear = nn.Linear(hidden_dim, vocab_size) # output size -> one-hot vector size (vocab size)\n","\n","    def forward(self, x):    # (batch_size, sequence_length)\n","        # 1(Embedding Layer)\n","        output = self.embedding_layer(x)    # →(batch_size, sequence_length, embedding_dim)\n","        # 2(RNN Layer)\n","        output, hidden = self.rnn_layer(output)     # → output(batch_size, sequence_length, hidden_dim), hidden (1, batch_size, hidden_dim)\n","        # 3(최종 출력 Layer)\n","        output = self.linear(output)    # →(batch_size, sequence_length, vocab_size)\n","        return output.view(-1, output.size(2))  # batch 차원 제거 (batch_size*sequence_length, vocab_size)"],"metadata":{"id":"0DbAJKX6lJWD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hyperparameter\n","vocab_size = len(word2index)  # <unk> 토큰 포함\n","embedding_dim = 5\n","hidden_dim = 16"],"metadata":{"id":"IO97JtTok2JQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Net(vocab_size, embedding_dim, hidden_dim, batch_first=True)\n","loss_function = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(params=model.parameters())"],"metadata":{"id":"W-fO3kvLlXA1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 임의의 예측(random initialized weight)\n","output = model(X)\n","print(output)\n","print(output.shape)"],"metadata":{"id":"KXOPo4GvlX0q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Train**"],"metadata":{"id":"TV6AOC-llb5Y"}},{"cell_type":"code","source":["for step in range(60):\n","    # gradient initialization\n","    optimizer.zero_grad()\n","    # forward\n","    output = model(X)\n","    # compute loss\n","    loss = loss_function(output, Y.view(-1))\n","    # backward\n","    loss.backward()\n","    # parameter update\n","    optimizer.step()\n","    if step % 5 == 0:\n","        print(\"[{:02d}/60] {:.4f} \".format(step+1, loss))\n","        pred = output.argmax(-1).tolist()\n","        print(\" \".join([\"Repeat\"] + [index2word.get(x) for x in pred]))\n","        print()"],"metadata":{"id":"UADfLlm6laSL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# HuggingFace를 사용해 Transforemr 구현해보기"],"metadata":{"id":"GB-c_ApklkBH"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FaV_RLHThsHD","executionInfo":{"status":"ok","timestamp":1700413555147,"user_tz":-540,"elapsed":14286,"user":{"displayName":"Jang Jang","userId":"00176447522705973821"}},"outputId":"ea76b769-be6a-4e37-cb59-a2b26553e173"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.5)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n","Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install transformers\n","!pip install datasets"]},{"cell_type":"markdown","source":["Trainer 클래스를 사용하지 않고, 2장과 같은 결과 얻는방법\n","* 다만, 섹션2에서 전처리를 완료했다고 가정\n","* 여기서 말하는 전처리는 Autotokenizer가 해주는 역할"],"metadata":{"id":"HC_8g0ljhucj"}},{"cell_type":"code","source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","\n","raw_datasets = load_dataset(\"glue\", \"mrpc\")\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","def tokenize_function(example):\n","    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n","\n","\n","tokenized_datasets = raw_datasets.map(tokenize_function, batched=True) # map 함수는 datasets 라이브러리의 내장함수로, 정의한 tokenize_function을 입력으로 받아 효율적으로 데이터셋을 tokenize 할 수 있게 만들어준다.\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"],"metadata":{"id":"Y7fOmLV1iMUl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenized_datasets)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jVU07c-FIVdb","executionInfo":{"status":"ok","timestamp":1700413556956,"user_tz":-540,"elapsed":10,"user":{"displayName":"Jang Jang","userId":"00176447522705973821"}},"outputId":"9a3c8eea-406e-4304-c54f-bc76dac4205a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 3668\n","    })\n","    validation: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 408\n","    })\n","    test: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 1725\n","    })\n","})\n"]}]},{"cell_type":"markdown","source":["#1. Training을 위한 준비\n","2에서 나왔던 Trainer Class인 API를 이용하지 않고 2에서 와 같이 구현해야 하기 때문에, 몇가지를 새롭게 정의해야 한다. 첫 번째는 배치(batch)를 반복하는 데 사용할 dataloaders입니다. 그러나 이 dataloaders를 정의하기 전에 Trainer가 자동으로 수행한 몇 가지 작업을 직접 처리하기 위해 tokenized_datasets에 약간의 후처리를 적용해야 합니다. 구체적으로 다음을 수행해야 합니다:\n","  * 1. tokenized_datasets에 대한 후처리\n","    * 1.1 모델이 필요로 하지 않는 값이 저장된 열(columns)을 제거합니다. (sentence1, sentence2 등)\n","    * 1.2 열 레이블(column label)의 이름을 labels로 바꿉니다. 이는 모델이 labels라는 이름으로 매개변수를 받기 때문입니다.\n","\n","    * 1.3 파이썬 리스트 대신 PyTorch 텐서(tensors)를 반환하도록 datasets의 형식을 설정합니다.\n","  * Autotokenizer의 역할중 encoding을 집적 해준다는 얘기이고, 이를 하는 이유는 model의 input으로 들어가는 조건을 맞추기 위해"],"metadata":{"id":"HCD35qyMXmdI"}},{"cell_type":"markdown","source":["tokenized_datasets에는 이러한 작업을 위한 별도의 메서드들이 존재합니다:"],"metadata":{"id":"Xg0h6TKCYtTp"}},{"cell_type":"code","source":["print('######아무것도 안건들인 원본 dataset#####')\n","print(tokenized_datasets.column_names)\n","#1.1 모델이 필요로 하지 않는 값이 저장된 열(columns)을 제거합니다. (sentence1, sentence2 등)\n","tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n","#1.2 label의 이름을 labels로 바꿔준다\n","tokenized_datasets = tokenized_datasets.rename_column('label','labels')\n","\n","print('################')\n","print(tokenized_datasets.column_names)\n","\n","# datasets.set_format(): datasets()에서 제공해주는 함수 : dataset을 torch.tensor 형태로 바꿔줌\n","print('################')\n","tokenized_datasets.set_format(\"torch\")\n","tokenized_datasets[\"train\"].column_names\n","\n","print(tokenized_datasets)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YdvMvSdYXp0U","executionInfo":{"status":"ok","timestamp":1700413556957,"user_tz":-540,"elapsed":10,"user":{"displayName":"Jang Jang","userId":"00176447522705973821"}},"outputId":"e4e99fd9-26c2-424b-9eef-6ba53bee0e37"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["######아무것도 안건들인 원본 dataset#####\n","{'train': ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'], 'validation': ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'], 'test': ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask']}\n","################\n","{'train': ['labels', 'input_ids', 'token_type_ids', 'attention_mask'], 'validation': ['labels', 'input_ids', 'token_type_ids', 'attention_mask'], 'test': ['labels', 'input_ids', 'token_type_ids', 'attention_mask']}\n","################\n","DatasetDict({\n","    train: Dataset({\n","        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 3668\n","    })\n","    validation: Dataset({\n","        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 408\n","    })\n","    test: Dataset({\n","        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n","        num_rows: 1725\n","    })\n","})\n"]}]},{"cell_type":"markdown","source":["위에서 보듯이 결과적으로 tokenized_datasets에는 모델이 허용하는 columns만으로 새롭게 구성했다(즉, Autotokenizer를 사용하지 않고서 만든 것)\n","\n","이제 이 작업이 완료되었으므로 dataloader를 쉽게 정의할 수 있다: </br>\n","* Dataloader: https://tutorials.pytorch.kr/beginner/basics/data_tutorial.html"],"metadata":{"id":"apvTZHYMb5eY"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","train_dataloader = DataLoader(\n","    tokenized_datasets['train'],\n","    shuffle=True,\n","    batch_size=8, # batch_size = mini_batch = 8    =>  1번의 batch당 8개의 raw_data씩 처리\n","                  # train_data갯수(3668) / 8 = 459번의 batch 처리 해야함  => 1-epoch에 459번의 iteration\n","    collate_fn=data_collator\n",")\n","\n","eval_dataloader = DataLoader(\n","    tokenized_datasets[\"validation\"],\n","    batch_size=8,\n","    collate_fn=data_collator\n",")"],"metadata":{"id":"fDsJPvMzb6YD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["데이터 처리에 오류가 없는지 빠르게 확인하기 위해 다음과 같이 배치(batch)를 검사할 수 있습니다:\n","* 1번째 방법. train_dataloader의 dictionary 형태를 이용해서 확인해보기\n","* 2번째 방법. Dataloader의 iter와 items"],"metadata":{"id":"piu6rEOze2OP"}},{"cell_type":"code","source":["# 1번째 방법. dictionary 형태를 이용해서 확인해보기\n","for one_batch in train_dataloader:\n","  # 한번의 batch만 포함시키기\n","  break\n","one_batch = {k: v for k,v in one_batch.items()}\n","one_batch\n","# token_type_ids : [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1] : \"0\": 첫번쨰 sequence의 token들 이다\n","#                                                                          \"1\": 두번째 sequence의 token들 이다\n","# 즉, 1raw당 2개의 sequence가 들어가 있다"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jmFjb4QEe18l","executionInfo":{"status":"ok","timestamp":1700413556957,"user_tz":-540,"elapsed":9,"user":{"displayName":"Jang Jang","userId":"00176447522705973821"}},"outputId":"45f00680-32ad-4830-9042-8ddc2c2e6e47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"execute_result","data":{"text/plain":["{'labels': tensor([1, 0, 1, 1, 1, 0, 1, 0]),\n"," 'input_ids': tensor([[  101,  1000,  1996, 10984,  1997,  1031,  1996, 13612,  1033,  2003,\n","           7078, 26233,  2000,  2033,  1010,  1000,  7912, 11531,  2056,  1999,\n","           1037,  4861,  2000,  4419,  2739,  3149,  2197,  2733,  1012,   102,\n","           7912, 11531,  3843,  1037,  4861,  2197,  2733,  2000,  1996,  4419,\n","           2739,  3149,  3038,  1010,  1000,  1996, 10984,  1997,  1031,  1996,\n","          13612,  1033,  2003,  7078, 26233,  2000,  2033,  1012,   102,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0],\n","         [  101,  2012, 22878,  6928,  1010,  1996,  2184,  1011,  2095,  3602,\n","           2149, 10790, 22123,  1027, 25269,  2018,  5707,  2260,  1013,  3590,\n","           1999,  3976,  3228,  1037, 10750,  1997,  1017,  1012,  2385,  3867,\n","           2013,  1017,  1012,  2260,  3867,  1012,   102,  2582,  2041,  1996,\n","           7774,  1010,  1996,  6847, 10665,  2184,  1011,  2095,  3602,  2149,\n","          10790, 22123,  1027, 25269,  8328,  2423,  1013,  3590,  1999,  3976,\n","           1010,  2635,  2049, 10750,  2000,  1017,  1012,  2656,  3867,  2013,\n","           1017,  1012,  2459,  3867,  1012,   102],\n","         [  101,  2002,  2056,  2008,  1999,  2220,  2285,  1010,  7521, 28421,\n","           2012,  7530, 15951,  2097,  2022,  2583,  2000,  2674,  1996, 27556,\n","           2000,  1996,  2976,  2231,  1005,  1055,  3274,  3550,  1000,  3422,\n","           7201,  1012,  1000,   102,  1999,  2220,  2285,  1010,  7521, 28421,\n","           2012,  7530, 15951,  2097,  2022,  2583,  2000,  2674,  1996, 27556,\n","           2000,  1996,  2976,  2231,  1005,  1055,  3274,  3550,  1000,  3422,\n","           7201,  1010,  1000,  3766,  2056,  1012,   102,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0],\n","         [  101,  2720,  1012,  2131,  9834, 20349,  2056,  2012,  2008,  2739,\n","           3034,  2008,  1000,  2057,  2024,  2183,  2000,  3613,  2000,  8691,\n","           2185,  2012,  1996,  7776,  2832,  2127,  2057,  3362,  2019,  3820,\n","           1010,  1000,  2007,  4811,  1998,  1043,  1012,  1049,  1012,   102,\n","           1000,  2057,  2024,  2183,  2000,  3613,  2000,  8691,  2185,  2012,\n","           1996,  7776,  2832,  2127,  2057,  3362,  2019,  3820,  1010,  1000,\n","           2002,  2056,  1012,   102,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0],\n","         [  101,  2002,  7356,  1996, 10420,  7226,  2004,  1000,  2012,  3217,\n","          18436,  2135,  2919,  5248,  2013,  1037,  2194,  2007,  1037,  2381,\n","           1997,  2012,  3217, 18436,  2135,  2919,  5248,  1012,  1000,   102,\n","          14783,  2170,  1996,  2693,  1000,  2012,  3217, 18436,  2135,  2919,\n","           9164,  2013,  1037,  2194,  2007,  1037,  2381,  1997,  2012,  3217,\n","          18436,  2135,  2919,  9164,  1012,  1000,   102,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0],\n","         [  101,  3988, 24534,  4311,  2265,  2027,  2351,  1997,  2139, 10536,\n","           7265,  3508,  1010, 23760, 12399, 10092,  1998, 10514,  4246, 23909,\n","           1012,   102,  2048,  2062,  2351,  2101,  1010,  1998,  3988, 24534,\n","           4311,  2265,  2027, 25642,  2000,  2139, 10536,  7265,  3508,  1010,\n","           1044, 22571, 14573,  2121, 10092,  1998, 10514,  4246, 23909,  1012,\n","            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0],\n","         [  101,  1996, 24294,  3077,  1011,  2241,  9121,  2194,  6406,  4848,\n","           5958,  1999,  2976,  2457,  1999, 17025,  1010,  3972,  1012,  1010,\n","           6815,  8731,  1011,  3976,  2003, 20084,  2049,  2687,  7353,  2006,\n","           9123,  4083,  2808,  2005,  6927, 12910,  1998, 23655,  2545,  1012,\n","            102,  1996, 24294,  3077,  1011,  2241,  9121,  2194,  6406,  4848,\n","           5958,  6815,  8731,  1011,  3976,  2003, 20084,  2049,  2687,  7353,\n","           2006,  9123,  1011,  4083,  2808,  2005,  6927, 12910,  1998, 23655,\n","           2545,  1012,   102,     0,     0,     0],\n","         [  101,  5262,  1010,  2720,  1012, 11417,  3363,  1010,  1037,  2969,\n","           1011,  2649,  2147,  4430, 23518,  1010,  2056,  2002,  3740,  2000,\n","           3961,  2200,  2920,  1999,  1996,  2194,  1012,   102,  2145,  1010,\n","           2720,  1012, 11417,  3363,  2097,  3961,  2200,  2172,  1037,  2373,\n","           2012,  1996,  2194,  1012,   102,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0]]),\n"," 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 0, 0, 0],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0]]),\n"," 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0]])}"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["# 2번째 방법. Dataloader의 iter()와 next()\n","data_iter = iter(train_dataloader)\n","print(next(data_iter))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24xmDq8-Epaq","executionInfo":{"status":"ok","timestamp":1700413556957,"user_tz":-540,"elapsed":8,"user":{"displayName":"Jang Jang","userId":"00176447522705973821"}},"outputId":"4c5f36ff-8c9d-4539-cdb0-059daedb3fa9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'labels': tensor([1, 0, 1, 0, 1, 1, 0, 1]), 'input_ids': tensor([[  101,  1999,  1037,  3661,  2741,  2000,  1996,  2335,  1998,  3191,\n","          2000,  1996,  3378,  2811,  2044,  2010,  8172,  1010, 10503, 11248,\n","          1000,  3167,  3314,  1000,  1998, 17806,  2005,  2010,  1000, 10876,\n","          2063,  1997,  4988,  2594, 11109,  1012,  1000,   102,  1999,  1037,\n","          3661,  2741,  2000,  1996,  2047,  2259,  2335,  2044,  2010,  8172,\n","          1010,  2720, 10503, 11248,  1000,  3167,  3314,  1000,  1998,  9706,\n","         12898, 17701,  2098,  2005,  2010,  1000, 10876,  2063,  1997,  4988,\n","          2594, 11109,  1000,  1012,   102],\n","        [  101,  2016,  2596,  1999,  2976,  2457,  2045,  6928,  1998,  2001,\n","          3517,  2000,  2022,  4015,  2000,  5395,  1999,  2048,  3134,  1012,\n","           102, 20977, 10795,  1999,  6044,  2006,  5958,  1998,  2001,  3517,\n","          2000,  2022,  4015,  2000,  5395,  1999,  2048,  3134,  1012,   102,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0],\n","        [  101,  1000,  2096,  2116,  2204,  2111,  2147,  1999,  1996, 10093,\n","         14545, 25074,  3436,  3068,  1010,  1000,  5747,  2056,  1999,  4810,\n","         12629,  1010,  1000,  1996,  2270,  2003,  3305,  8231,  3974, 11752,\n","          2007,  2122, 18162,  3042,  4455,  1010, 18162, 24554,  2015,  1012,\n","           102,  2096,  2002,  2056,  1000,  2116,  2204,  2111,  2147,  1999,\n","          1996, 10093, 14545, 25074,  3436,  3068,  1010,  1996,  2270,  2003,\n","          3305,  8231,  3974, 11752,  2007,  2122, 18162,  3042,  4455,  1010,\n","         18162, 24554,  2015,  1012,   102],\n","        [  101,  1000,  2057,  2024,  7537,  2007,  1996,  3648,  1005,  1055,\n","          3247,  1998,  7537,  2008,  2027,  3970,  2256,  9918,  1010,  1000,\n","          1037, 16239, 14056,  2056,  9857,  1012,   102,  1000,  2057,  1005,\n","          2128,  7537,  2007,  1996,  3648,  1005,  1055,  3247,  1010,  1000,\n","          2056,  2928, 23506,  1010, 14056,  2005, 16239, 11404,  1012,   102,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0],\n","        [  101,  1996,  2450,  1010,  2984, 20484,  4679,  1010,  4583,  1010,\n","          2001,  4727,  2011,  1996,  2110,  2610,  2006, 13292,  1012,  2322,\n","          1998,  5338,  2007,  2034,  1011,  3014,  2474, 19170,  4890,  1012,\n","           102,  2984, 20484,  4679,  1010,  4583,  1010,  1997,  2676,  7614,\n","          2346,  1010, 18243,  9013,  1010,  2001,  4727, 13292,  1012,  2322,\n","          2011,  2110,  2610,  1998,  5338,  2007,  2034,  1011,  3014,  2474,\n","         19170,  4890,  1012,   102,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0],\n","        [  101,  6627, 15768,  2020,  3480,  2011,  1037, 14768, 19939,  2013,\n","          3103, 12702, 29390,  1010,  2029,  2001,  7021,  2004,  1037,  2919,\n","         18168,  2368,  2005,  1996,  9046, 12174, 16565,  2161,  1012,   102,\n","          1037, 14768, 19939,  2013,  3103, 12702, 29390,  4297,  1012,  3103,\n","          2860,  1012,  1051,  2404,  2062,  3778,  2006,  2813,  2395,  2077,\n","          1996, 12174, 16565,  2161,  1012,   102,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0],\n","        [  101,  2006,  5553,  1012,  2184,  1010,  2322,  1011,  2095,  1011,\n","          2214,  8233, 13832, 15339, 23680, 13911,  3334,  1010,  2040,  2036,\n","          2001,  6875,  1010,  2001,  2179,  5391,  1998, 21384,  1999,  2014,\n","          4545,  1999,  1996,  2168,  5101,  1012,   102,  2006,  5553,  1012,\n","          1015,  1010, 23205,  2401,  8683,  3363, 11382, 14277,  4017, 11285,\n","          1010,  2040,  2001,  6875,  1010,  2001,  2179,  5391,  1998, 21384,\n","          1999,  2014,  4545,  1012,   102,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0],\n","        [  101,  1996,  2630, 19392,  9033,  2290,  2081,  2049,  8874,  2012,\n","          1996,  2630, 19392,  2088,  3519,  1999,  7598,  2023,  2733,  1012,\n","           102,  1037,  2047,  2544,  1997,  1996,  2630, 19392, 12827,  2001,\n","          3985,  3390,  2651,  2012,  1996,  2707,  1997,  1996,  2630, 19392,\n","          2088,  3519,  1999,  7598,  1012,   102,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0]])}\n"]}]},{"cell_type":"code","source":["print(tokenizer.decode(one_batch['input_ids'][0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m_cLZGBt05Fx","executionInfo":{"status":"ok","timestamp":1700413556957,"user_tz":-540,"elapsed":6,"user":{"displayName":"Jang Jang","userId":"00176447522705973821"}},"outputId":"8767418e-fba0-4d7b-ab69-1c23e5848d6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS] \" the timing of [ the miniseries ] is absolutely staggering to me, \" nancy reagan said in a statement to fox news channel last week. [SEP] nancy reagan issued a statement last week to the fox news channel saying, \" the timing of [ the miniseries ] is absolutely staggering to me. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"]}]},{"cell_type":"markdown","source":["실제 데이터 형태(shapes)가 살짝 다를 수 있는데 이는 학습 dataloader에 대해 shuffle=True를 설정하고 배치(batch) 내에서의 최대 길이로 패딩(padding)하기 때문입니다.\n","  * 즉, iteration마다 data가 suffle되고 섞인것에 맞춰서 padding이 채워지니까"],"metadata":{"id":"WN-6ylio2KN_"}},{"cell_type":"code","source":["#----------------------전처리 끝----------------------#"],"metadata":{"id":"PYV-IoMk19RG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForSequenceClassification\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xq_uS5xQgFUe","executionInfo":{"status":"ok","timestamp":1700413558829,"user_tz":-540,"elapsed":1876,"user":{"displayName":"Jang Jang","userId":"00176447522705973821"}},"outputId":"822583c5-22c8-4bde-fda9-1b273ac46d0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","source":["학습 과정에서 모든 것들이 원활하게 진행될 수 있는지 확인하기 위해 배치(batch)를 이 모델에 한번 전달해 봅시다:"],"metadata":{"id":"y3Rmvy1zhhPu"}},{"cell_type":"code","source":["# 여기서는 one_batch만 들어가는 형태이다(즉, 모든 data에 대해서 loss를 구한게 아니라 minibatch=8에 대해서만 loss구한거임)\n","outputs = model(**one_batch) # **: data가 dict 형태니까 그 중에서 value값을 넣겠다\n","                             # * : dict형태에서 key값을 넣겠다\n","                             # 즉, 여기서는 label과 같이 들어갔으니까 loss도 같이 나온거임\n","print(outputs)\n","print(outputs.loss, outputs.logits.shape)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mvu3vBqDhZvd","executionInfo":{"status":"ok","timestamp":1700413559860,"user_tz":-540,"elapsed":1033,"user":{"displayName":"Jang Jang","userId":"00176447522705973821"}},"outputId":"baf23238-b174-4ac0-bffb-a1eca429d0c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SequenceClassifierOutput(loss=tensor(0.7505, grad_fn=<NllLossBackward0>), logits=tensor([[0.4054, 0.0935],\n","        [0.4053, 0.0717],\n","        [0.4246, 0.0746],\n","        [0.4213, 0.0886],\n","        [0.4192, 0.0664],\n","        [0.3954, 0.0817],\n","        [0.4038, 0.0960],\n","        [0.4041, 0.1022]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n","tensor(0.7505, grad_fn=<NllLossBackward0>) torch.Size([8, 2])\n"]}]},{"cell_type":"code","source":["from transformers import AdamW\n","\n","optimizer = AdamW(model.parameters(), lr=5e-5)"],"metadata":{"id":"jzi_DV_6lFe6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["model의 parameter 확인 방법\n","* 모델의 파라미터를 확인 하는 방법은 model.parameters()를 통해 가능합니다. 단, model.parameters()는 generator 타입이므로 for문과 같이 순회하면서 또는 next를 이용하여 값을 접근할 수 있다."],"metadata":{"id":"c8d9ObKLbF0t"}},{"cell_type":"code","source":["#1번째 방법: model.parameters()는 generator 형태이므로, iter형태이기 때문에 next()로 접근이 가능\n","print(next(model.parameters()))\n","print(next(model.parameters()).shape)\n","\n","#2번째 방법: model.parameters()는 generator 형태이므로, iter형태이기 때문에 for문으로 접근 가능\n","print('#-------------------#')\n","for param in model.parameters():\n","  print(param)\n","  print(param.shape)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PpZSWqgaaqlp","executionInfo":{"status":"ok","timestamp":1700413559860,"user_tz":-540,"elapsed":7,"user":{"displayName":"Jang Jang","userId":"00176447522705973821"}},"outputId":"a68e72c6-c501-4835-e017-2ac4f4d9bbce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[-0.0102, -0.0615, -0.0265,  ..., -0.0199, -0.0372, -0.0098],\n","        [-0.0117, -0.0600, -0.0323,  ..., -0.0168, -0.0401, -0.0107],\n","        [-0.0198, -0.0627, -0.0326,  ..., -0.0165, -0.0420, -0.0032],\n","        ...,\n","        [-0.0218, -0.0556, -0.0135,  ..., -0.0043, -0.0151, -0.0249],\n","        [-0.0462, -0.0565, -0.0019,  ...,  0.0157, -0.0139, -0.0095],\n","        [ 0.0015, -0.0821, -0.0160,  ..., -0.0081, -0.0475,  0.0753]],\n","       requires_grad=True)\n","torch.Size([30522, 768])\n","#-------------------#\n","Parameter containing:\n","tensor([[-0.0102, -0.0615, -0.0265,  ..., -0.0199, -0.0372, -0.0098],\n","        [-0.0117, -0.0600, -0.0323,  ..., -0.0168, -0.0401, -0.0107],\n","        [-0.0198, -0.0627, -0.0326,  ..., -0.0165, -0.0420, -0.0032],\n","        ...,\n","        [-0.0218, -0.0556, -0.0135,  ..., -0.0043, -0.0151, -0.0249],\n","        [-0.0462, -0.0565, -0.0019,  ...,  0.0157, -0.0139, -0.0095],\n","        [ 0.0015, -0.0821, -0.0160,  ..., -0.0081, -0.0475,  0.0753]],\n","       requires_grad=True)\n","torch.Size([30522, 768])\n"]}]},{"cell_type":"markdown","source":["마지막으로, Trainer에서 디폴트로 사용되는 학습률 스케줄러(learning rate scheduler)는 최대값(5e-5)에서 0까지 선형 감쇠(linear decay)합니다. 이를 적절하게 정의하려면 우리가 수행할 학습 단계의 횟수를 알아야 합니다. 이는 실행하려는 에포크(epochs) 수에 학습 배치(batch)의 개수를 곱한 것입니다. 학습 배치의 개수는 학습 dataloader의 길이와 같습니다. Trainer는 디폴트로 3개의 에포크(epochs)를 사용하므로 다음을 따릅니다:\n","* 1epoch에 batch_size만큼의 weight이 update, 전체적으로 보면 weight은 1번씩 update된것\n","* batch의 갯수 = dataloader의 길이(len(train_dataloader))\n","* warmup_step : learning rate가 0.01이라고 한다면 처음 10 step 동안은 0.001, 0.002, 0.003 ~ 0.01까지 선형적으로 조금씩만 증가하는 learning rate을 사용하는 것입니다. 이는 반대로 말하면 샘플이 적은 초기에 아주아주 작은 learning rate을 사용함으로써 bad local optima 로의 학습이 일어나지 않게 만든다\n","  * https://zzaebok.github.io/deep_learning/RAdam/"],"metadata":{"id":"sfRV_JEXlMwZ"}},{"cell_type":"code","source":["print(len(train_dataloader)) #batch의 갯수\n","                             # 즉, 459이면 batch의 갯수가 459이다\n","                             # 즉 , batch를 459번 반복한다 = 459 iteration = 1epoch\n","                             # 여기선 batch_size(mini_batch) = 8 이었고, train_data=3668 이니까\n","                             # 3668/8(batch_size) = 459 = 459번의 batch = 459 iteration"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qte0UWLTliq9","executionInfo":{"status":"ok","timestamp":1700413559860,"user_tz":-540,"elapsed":6,"user":{"displayName":"Jang Jang","userId":"00176447522705973821"}},"outputId":"c7be5e97-9a36-4658-b940-8671feccbf14"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["459\n"]}]},{"cell_type":"code","source":["from transformers import get_scheduler\n","\n","num_epochs = 3 #train_Data전체를 3 epoch으로 train\n","num_training_steps = num_epochs * len(train_dataloader) # len(train_dataloader) : 1epoch당 batch의 갯수\n","                                                        # 1377\n","lr_scheduler = get_scheduler(\n","    'linear', # The name of the scheduler to use.\n","    optimizer = optimizer,\n","    num_warmup_steps=0,            #Q)warmup_step을 이용해 초반의 step 동안은 0.001, 0.002, 0.003 ~ 0.01까지 linearly하게 증가하게 사용한다. 이값을 설정하는게 어떤 의미가 있을까?\n","                                   # Hugging face에서는 warmup step의 수라고 한다..\n","                                   # 그렇다면, num_warmup_steps=10이라면, 10 step동안은 Linearly하게 learning rate를 Linearly하게 증가시킨 후 .. 일정하게 사용하는 느낌?\n","                                   # Reference : https://huggingface.co/docs/transformers/main_classes/optimizer_schedules\n","    num_training_steps=num_training_steps # 1377\n",")"],"metadata":{"id":"nhhyVZQyn6-Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#2. Training Loop"],"metadata":{"id":"KgvC6ju0pe-p"}},{"cell_type":"markdown","source":["model과 배치(batch)를 적재할 device를 정의한다."],"metadata":{"id":"Ct9cWKdQk3W0"}},{"cell_type":"code","source":["import torch\n","\n","#device = torch.device(\"cuda:0\")  # 0번쨰 gpu로 device를 사용하겠다\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","model.to(device) #설정된 device에 model할당\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WjVtZsAyphmh","executionInfo":{"status":"ok","timestamp":1700413559860,"user_tz":-540,"elapsed":5,"user":{"displayName":"Jang Jang","userId":"00176447522705973821"}},"outputId":"642e08b0-9202-4d79-d8e8-9eeee4fdb441"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","source":["학습이 언제 끝날지 정보를 얻기 위해 tqdm 라이브러리를 사용하여 학습 단계(training steps)를 기준으로 진행 표시줄(progress bar)을 출력할 수 있도록 합니다:\n","https://gaussian37.github.io/dl-pytorch-snippets/#optimizerzero_grad-lossbackward-optimizerstep-1 참고"],"metadata":{"id":"wsYnK6MN39aS"}},{"cell_type":"code","source":["from tqdm.auto import tqdm\n","\n","progress_bar = tqdm(range(num_training_steps))\n","\n","model.train() # model.train() : 단지 model한테 학습할거라고 알려주는 것 뿐이다\n","'''\n","#앞전에 정의했던 dataloader\n","train_dataloader = DataLoader(\n","    tokenized_datasets['train'],\n","    shuffle=True,\n","    batch_size=8, # batch_size = mini_batch = 8    =>  1번의 batch당 8개의 raw_data씩 처리\n","                  # train_data갯수(3668) / 8 = 459번의 batch 처리 해야함  => 1-epoch에 459번의 iteration\n","    collate_fn=data_collator\n",")\n","'''\n","\n","for epoch in range(num_epochs):\n","    #1번의 epoch에 459번의 batch처리\n","    for batch in train_dataloader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        loss.backward()\n","\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad() # batch가 끝나고, 다시 반복할대마다 새로운 gradient를 계산해야 하므로, 새롭게 초기화\n","        progress_bar.update(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["fbc07594bc874f2cb160bc1e01079c48","a64715b8f48a4c8da41a01865130fdad","db931f21783d47eab023fb005eb6d707","3ac5b265ab5a4304abfcdf22701ea44a","77e4bdc74ca94d7da3f66470d14ed7e7","c0860302d87b484da331082547d90bdf","dd32680f5beb4ad49af35f75a33faa3f","9a7e73438c924e73884d7352611782ed","b8a48b237f754c71a9bb5bdef0528167","c01c5ef492dc483db1145de6d21f81a3","81b8bb88a827494cbfa078c363959287"]},"id":"ERBXkg9ep6AH","executionInfo":{"status":"ok","timestamp":1700413734448,"user_tz":-540,"elapsed":174592,"user":{"displayName":"Jang Jang","userId":"00176447522705973821"}},"outputId":"904959fe-7722-4b08-c0ca-599a45a56b8f"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1377 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbc07594bc874f2cb160bc1e01079c48"}},"metadata":{}}]},{"cell_type":"markdown","source":["# Evaluation Loop\n"],"metadata":{"id":"tK0hBw660Tp6"}},{"cell_type":"code","source":["from datasets import load_metric\n","\n","metric = load_metric(\"glue\", \"mrpc\") # evaluation metric\n","\n","model.eval() # model을 evaluation 모드로 변환\n","for batch in eval_dataloader:\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","\n","    logits = outputs.logits\n","    predictions = torch.argmax(logits, dim=-1)\n","    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n","\n","# evaluation 결과계산 및 출력\n","metric.compute()"],"metadata":{"id":"v6vxCu3nzxd7","executionInfo":{"status":"ok","timestamp":1700413737842,"user_tz":-540,"elapsed":3401,"user":{"displayName":"Jang Jang","userId":"00176447522705973821"}},"colab":{"base_uri":"https://localhost:8080/","height":105,"referenced_widgets":["0d9d896ac5574105af7b64d6805150b2","191a4b58634b4145a832347ea71121f0","f99ec136d3c843cab7a9cfb151696e03","a90b621f3a8846e5a2c5aa30b56cbe70","de6118d5ece14045abbe0a1607531451","f8a27c82a0514500afe7280affb60243","31ccaf8b70ec4e7681393b63da163550","e64fca755860403daee1d9cdbea127f3","56979b6301a54831ac09975641747f17","40e8246ae67949709ef035a9885bf233","6f060530ed0045afacc853c0a1f3dfe3"]},"outputId":"8bebd54b-2dda-4e5c-8161-e71f4d51f15a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-36-5fc8e56794ae>:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","  metric = load_metric(\"glue\", \"mrpc\") # evaluation metric\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d9d896ac5574105af7b64d6805150b2"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 0.8602941176470589, 'f1': 0.9015544041450777}"]},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","source":["evaluation시, with torch.no_grad() 쓰는 이유:  https://coffeedjimmy.github.io/pytorch/2019/11/05/pytorch_nograd_vs_train_eval/\n"],"metadata":{"id":"SIyQ_QC3srrl"}}]}